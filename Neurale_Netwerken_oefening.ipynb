{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRb8dxfSc/KCI+UteehG9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinekescode/NeuraleNetwerken/blob/main/Neurale_Netwerken_oefening.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neurale Netwerken\n",
        "\n",
        "Wat je leert in deze cursus:\n",
        "1. Hoe je packages moet installeren\n",
        "2. Basis python concepten\n",
        "3. Inladen van een dataset en deze visualiseren\n",
        "4. Hoe bouw ik een Neuraal Netwerk\n",
        "5. Train het netwerk op onze dataset en test of het werkt\n",
        "\n",
        "## 1. Packages installeren"
      ],
      "metadata": {
        "id": "S3NDd9l7jljI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment these lines if you're in Google Colab or don't have packages installed:\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install scikit-learn\n",
        "# !pip install seaborn\n",
        "# !pip install pandas\n",
        "\n",
        "# If you see any errors above, that's normal - it means the packages are already installed."
      ],
      "metadata": {
        "id": "kNgHVGpEQM05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1UCAyiPi5T1"
      },
      "outputs": [],
      "source": [
        "# Basic math and data handling\n",
        "import numpy as np              # For mathematical operations\n",
        "import pandas as pd            # For data manipulation (like Excel for Python)\n",
        "import matplotlib.pyplot as plt # For creating graphs and charts\n",
        "import seaborn as sns          # For beautiful statistical plots\n",
        "\n",
        "# Set up plotting\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "# Machine learning tools\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Make plots appear in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"All packages imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Basis Python concepten\n",
        "Voor degenen die nog nooit Python code gezien hebben hier de korte basis Python voor Dummies:"
      ],
      "metadata": {
        "id": "OUMVSMvzjtnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables - like containers that hold data\n",
        "my_name = \"Neural Network Student\"\n",
        "my_age = 25\n",
        "print(f\"Hello! I'm {my_name} and I'm {my_age} years old.\")\n",
        "\n",
        "# Lists - collections of items\n",
        "fruits = [\"apple\", \"banana\", \"orange\"]\n",
        "print(f\"My favorite fruits: {fruits}\")\n",
        "\n",
        "# Numpy arrays - like lists but for math\n",
        "numbers = np.array([1, 2, 3, 4, 5])\n",
        "print(f\"Numbers: {numbers}\")\n",
        "print(f\"Numbers squared: {numbers**2}\")\n",
        "\n",
        "# Dictionaries - store information with labels\n",
        "person = {\n",
        "    \"name\": \"Alice\",\n",
        "    \"age\": 30,\n",
        "    \"city\": \"New York\"\n",
        "}\n",
        "print(f\"Person info: {person}\")\n",
        "print(f\"Person name: {person['name']}\")"
      ],
      "metadata": {
        "id": "AB6pXgvNjwsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inladen en onderzoeken van een dataset\n",
        "\n",
        "Hieronder gaan we de dataset van iris bloemen inlezen die we gaan gebruiken om ons model te trainen en testen. In deze dataset zitten 150 bloemen van 3 verschillende soorten."
      ],
      "metadata": {
        "id": "uIx8xw8clf7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "print(\"‚úÖ Dataset loaded successfully!\")\n",
        "\n",
        "# Let's explore what we have\n",
        "print(f\"\\nDataset shape: {iris.data.shape}\")\n",
        "print(f\"This means: {iris.data.shape[0]} flowers, {iris.data.shape[1]} measurements per flower\")\n",
        "\n",
        "print(f\"\\nFlower species in our dataset:\")\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    print(f\"  {i}: {species}\")\n",
        "\n",
        "print(f\"\\nMeasurements we have for each flower:\")\n",
        "for i, feature in enumerate(iris.feature_names):\n",
        "    print(f\"  {i}: {feature}\")"
      ],
      "metadata": {
        "id": "x5-2wB1il1gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a more user-friendly format\n",
        "X = iris.data  # The measurements (features)\n",
        "y = iris.target  # The species (what we want to predict)\n",
        "\n",
        "# Create a DataFrame - like a spreadsheet in Python\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = [iris.target_names[i] for i in y]\n",
        "\n",
        "print(\"First 10 flowers in our dataset:\")\n",
        "print(df.head(10))\n",
        "\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"Total flowers: {len(df)}\")\n",
        "print(f\"Flowers per species:\")\n",
        "print(df['species'].value_counts())\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nBasic statistics for our measurements:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "7KMAncK4l82x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with multiple subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Iris Dataset Exploration', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Histogram of sepal length\n",
        "axes[0, 0].hist(df['sepal length (cm)'], bins=15, alpha=0.7, color='skyblue')\n",
        "axes[0, 0].set_title('Distribution of Sepal Length')\n",
        "axes[0, 0].set_xlabel('Sepal Length (cm)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 2. Scatter plot: Sepal length vs Sepal width\n",
        "colors = ['red', 'green', 'blue']\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    mask = df['species'] == species\n",
        "    axes[0, 1].scatter(df[mask]['sepal length (cm)'],\n",
        "                      df[mask]['sepal width (cm)'],\n",
        "                      c=colors[i], label=species, alpha=0.7)\n",
        "axes[0, 1].set_title('Sepal Length vs Sepal Width')\n",
        "axes[0, 1].set_xlabel('Sepal Length (cm)')\n",
        "axes[0, 1].set_ylabel('Sepal Width (cm)')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Box plot of petal length by species\n",
        "df.boxplot(column='petal length (cm)', by='species', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Petal Length by Species')\n",
        "axes[1, 0].set_xlabel('Species')\n",
        "axes[1, 0].set_ylabel('Petal Length (cm)')\n",
        "\n",
        "# 4. Correlation heatmap\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "correlation_matrix = numeric_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Feature Correlations')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nDENT385mDNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Hoe bouw ik een Neuraal Netwerk\n",
        "\n",
        "In de onderstaande blokken gaan we daadwerkelijk een Neuraal Netwerk opzetten, trainen en testen."
      ],
      "metadata": {
        "id": "Nt8dJ8t2QzQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "# Think of this as: some flowers to learn from, some flowers to test on\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,      # Use 30% for testing\n",
        "    random_state=42,    # For reproducible results\n",
        "    stratify=y          # Keep equal proportions of each species\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} flowers\")\n",
        "print(f\"Testing set: {X_test.shape[0]} flowers\")"
      ],
      "metadata": {
        "id": "yPU50WqlQ_42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the distribution in our splits\n",
        "print(f\"\\nTraining set species distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for i, count in enumerate(counts):\n",
        "    print(f\"  {iris.target_names[i]}: {count} flowers\")\n",
        "\n",
        "print(f\"\\nTesting set species distribution:\")\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "for i, count in enumerate(counts):\n",
        "    print(f\"  {iris.target_names[i]}: {count} flowers\")\n"
      ],
      "metadata": {
        "id": "dj3UdKQoRVbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features (make them all have similar ranges)\n",
        "# This is like converting all measurements to the same scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\n‚úÖ Data preparation complete!\")\n",
        "print(f\"Original feature ranges:\")\n",
        "print(f\"  Sepal length: {X_train[:, 0].min():.2f} to {X_train[:, 0].max():.2f}\")\n",
        "print(f\"  Sepal width:  {X_train[:, 1].min():.2f} to {X_train[:, 1].max():.2f}\")\n",
        "print(f\"  Petal length: {X_train[:, 2].min():.2f} to {X_train[:, 2].max():.2f}\")\n",
        "print(f\"  Petal width:  {X_train[:, 3].min():.2f} to {X_train[:, 3].max():.2f}\")\n",
        "\n",
        "print(f\"\\nScaled feature ranges:\")\n",
        "print(f\"  All features now have mean ‚âà 0 and standard deviation ‚âà 1\")\n",
        "print(f\"  This helps our neural network learn more efficiently!\")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "_HQQwfsJRZ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hieronder een stukje wiskunde die nodig is om het Neurale Netwerk straks te kunnen trainen:"
      ],
      "metadata": {
        "id": "tLJKlAKrTMK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"\n",
        "    These are the mathematical functions that help neurons make decisions.\n",
        "    Think of them as the neuron's way of deciding how excited it should be!\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        \"\"\"\n",
        "        Sigmoid function: squashes any number to between 0 and 1\n",
        "        Like a smooth on/off switch for neurons\n",
        "        \"\"\"\n",
        "        # Prevent overflow by clipping extreme values\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(x):\n",
        "        \"\"\"\n",
        "        Derivative of sigmoid - needed for learning\n",
        "        \"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        \"\"\"\n",
        "        Softmax function: converts numbers to probabilities\n",
        "        Perfect for classification (which flower species?)\n",
        "        \"\"\"\n",
        "        # Subtract max for numerical stability\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Let's visualize these functions\n",
        "x = np.linspace(-10, 10, 100)\n",
        "sigmoid_y = ActivationFunctions.sigmoid(x)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Sigmoid function\n",
        "ax1.plot(x, sigmoid_y, 'b-', linewidth=2, label='Sigmoid')\n",
        "ax1.set_title('Sigmoid Activation Function')\n",
        "ax1.set_xlabel('Input')\n",
        "ax1.set_ylabel('Output')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.text(0, 0.2, 'Maps any number\\nto 0-1 range', ha='center',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
        "\n",
        "# Sigmoid derivative\n",
        "sigmoid_deriv = ActivationFunctions.sigmoid_derivative(sigmoid_y)\n",
        "ax2.plot(x, sigmoid_deriv, 'r-', linewidth=2, label='Sigmoid Derivative')\n",
        "ax2.set_title('Sigmoid Derivative (for learning)')\n",
        "ax2.set_xlabel('Input')\n",
        "ax2.set_ylabel('Derivative')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.text(0, 0.05, 'Tells us how much\\nto adjust weights', ha='center',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KScAw6oYTKbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nu we de data hebben gaan we echt een Neuraal Netwerk bouwen."
      ],
      "metadata": {
        "id": "NZJMZePGSOzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"\n",
        "    Our first neural network! üß†\n",
        "\n",
        "    Architecture:\n",
        "    - Input layer: 4 neurons (for 4 flower measurements)\n",
        "    - Hidden layer: 8 neurons (the brain of our network)\n",
        "    - Output layer: 3 neurons (for 3 flower species)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size=4, hidden_size=8, output_size=3, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initialize our neural network with random weights\n",
        "\n",
        "        Think of weights as the strength of connections between neurons\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights randomly (small values work better)\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.1\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.1\n",
        "\n",
        "        # Initialize biases to zero\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "        # Keep track of training history\n",
        "        self.training_history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        print(f\"‚úÖ Neural Network Created!\")\n",
        "        print(f\"   Input layer: {input_size} neurons\")\n",
        "        print(f\"   Hidden layer: {hidden_size} neurons\")\n",
        "        print(f\"   Output layer: {output_size} neurons\")\n",
        "        print(f\"   Learning rate: {learning_rate}\")\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass: data flows from input to output\n",
        "        This is how the network makes predictions!\n",
        "        \"\"\"\n",
        "        # Input to hidden layer\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = ActivationFunctions.sigmoid(self.hidden_input)\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = ActivationFunctions.softmax(self.output_input)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward_pass(self, X, y, output):\n",
        "        \"\"\"\n",
        "        Backward pass: learning happens here!\n",
        "        The network adjusts its weights based on mistakes\n",
        "        \"\"\"\n",
        "        # Convert y to one-hot encoding\n",
        "        y_onehot = np.zeros((len(y), 3))\n",
        "        y_onehot[np.arange(len(y)), y] = 1\n",
        "\n",
        "        # Calculate error at output layer\n",
        "        output_error = output - y_onehot\n",
        "\n",
        "        # Calculate gradients for output layer\n",
        "        d_weights_hidden_output = np.dot(self.hidden_output.T, output_error) / len(X)\n",
        "        d_bias_output = np.sum(output_error, axis=0, keepdims=True) / len(X)\n",
        "\n",
        "        # Calculate error at hidden layer\n",
        "        hidden_error = np.dot(output_error, self.weights_hidden_output.T) * \\\n",
        "                      ActivationFunctions.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Calculate gradients for hidden layer\n",
        "        d_weights_input_hidden = np.dot(X.T, hidden_error) / len(X)\n",
        "        d_bias_hidden = np.sum(hidden_error, axis=0, keepdims=True) / len(X)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output -= self.learning_rate * d_weights_hidden_output\n",
        "        self.bias_output -= self.learning_rate * d_bias_output\n",
        "        self.weights_input_hidden -= self.learning_rate * d_weights_input_hidden\n",
        "        self.bias_hidden -= self.learning_rate * d_bias_hidden\n",
        "\n",
        "    def train(self, X, y, epochs=1000, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the neural network!\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward_pass(X)\n",
        "\n",
        "            # Calculate loss (cross-entropy)\n",
        "            y_onehot = np.zeros((len(y), 3))\n",
        "            y_onehot[np.arange(len(y)), y] = 1\n",
        "            loss = -np.mean(np.sum(y_onehot * np.log(output + 1e-8), axis=1))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = np.argmax(output, axis=1)\n",
        "            accuracy = np.mean(predictions == y)\n",
        "\n",
        "            # Store history\n",
        "            self.training_history['loss'].append(loss)\n",
        "            self.training_history['accuracy'].append(accuracy)\n",
        "\n",
        "            # Backward pass (learning!)\n",
        "            self.backward_pass(X, y, output)\n",
        "\n",
        "            # Print progress\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Training complete! Final accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions on new data\n",
        "        \"\"\"\n",
        "        output = self.forward_pass(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Visualize how the network learned over time\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Plot loss\n",
        "        ax1.plot(self.training_history['loss'], 'b-', linewidth=2)\n",
        "        ax1.set_title('Training Loss Over Time')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax2.plot(self.training_history['accuracy'], 'g-', linewidth=2)\n",
        "        ax2.set_title('Training Accuracy Over Time')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "print(\"‚úÖ Neural Network class created! Ready to train!\")"
      ],
      "metadata": {
        "id": "c-X0VZowSTG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Train het netwerk op onze dataset en test of het werkt"
      ],
      "metadata": {
        "id": "9tDxhkmIUpLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the network\n",
        "nn = SimpleNeuralNetwork(input_size=4, hidden_size=8, output_size=3, learning_rate=0.1)\n",
        "\n",
        "# Train the network\n",
        "nn.train(X_train, y_train, epochs=1000, verbose=True)\n",
        "\n",
        "# Visualize the training process\n",
        "nn.plot_training_history()"
      ],
      "metadata": {
        "id": "38uDDNM9S5ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data\n",
        "test_predictions = nn.predict(X_test)\n",
        "train_predictions = nn.predict(X_train)\n",
        "\n",
        "# Calculate accuracies\n",
        "train_accuracy = np.mean(train_predictions == y_train)\n",
        "test_accuracy = np.mean(test_predictions == y_test)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.1f}%)\")\n",
        "print(f\"Testing Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
        "\n",
        "# Species names for better understanding\n",
        "species_names = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "# Show some example predictions\n",
        "print(f\"\\n=== EXAMPLE PREDICTIONS ===\")\n",
        "for i in range(min(10, len(X_test))):\n",
        "    actual = species_names[y_test[i]]\n",
        "    predicted = species_names[test_predictions[i]]\n",
        "    status = \"‚úÖ\" if y_test[i] == test_predictions[i] else \"‚ùå\"\n",
        "    print(f\"Flower {i+1}: Actual = {actual:12s}, Predicted = {predicted:12s} {status}\")\n"
      ],
      "metadata": {
        "id": "WIf5t6aRTnVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nog wat uitleg hoe we kunnen zien wat het netwerk geleerd heeft:\n"
      ],
      "metadata": {
        "id": "nc4J72PuUACB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== UNDERSTANDING WHAT THE NETWORK LEARNED ===\")\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, test_predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=species_names, yticklabels=species_names)\n",
        "plt.title('Confusion Matrix - How Well Did Our Network Do?')\n",
        "plt.xlabel('Predicted Species')\n",
        "plt.ylabel('Actual Species')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Confusion matrix shows where our network made mistakes!\")\n",
        "\n",
        "# Visualize the network's weights\n",
        "def visualize_network_weights(nn):\n",
        "    \"\"\"\n",
        "    Visualize what the network learned\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Input to hidden weights\n",
        "    im1 = ax1.imshow(nn.weights_input_hidden.T, cmap='coolwarm', aspect='auto')\n",
        "    ax1.set_title('Input to Hidden Layer Weights')\n",
        "    ax1.set_xlabel('Input Features')\n",
        "    ax1.set_ylabel('Hidden Neurons')\n",
        "    ax1.set_xticks(range(4))\n",
        "    ax1.set_xticklabels(['Sepal\\nLength', 'Sepal\\nWidth', 'Petal\\nLength', 'Petal\\nWidth'])\n",
        "    plt.colorbar(im1, ax=ax1)\n",
        "\n",
        "    # Hidden to output weights\n",
        "    im2 = ax2.imshow(nn.weights_hidden_output.T, cmap='coolwarm', aspect='auto')\n",
        "    ax2.set_title('Hidden to Output Layer Weights')\n",
        "    ax2.set_xlabel('Hidden Neurons')\n",
        "    ax2.set_ylabel('Output Classes')\n",
        "    ax2.set_yticks(range(3))\n",
        "    ax2.set_yticklabels(species_names)\n",
        "    plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_network_weights(nn)\n",
        "print(\"‚úÖ These heatmaps show the strength of connections in your network!\")"
      ],
      "metadata": {
        "id": "V-DcCUHCUD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gebruik het netwerk\n",
        "Nu het netwerk getraind en getest is kan het gebruikt worden om nieuwe data in een categorie te plaatsen (de reden van het testen van een netwerk)."
      ],
      "metadata": {
        "id": "gluV9bXCUO6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_flower(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    \"\"\"\n",
        "    Predict the species of a new flower\n",
        "    \"\"\"\n",
        "    # Create input array\n",
        "    new_flower = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
        "\n",
        "    # We need to scale it the same way as our training data\n",
        "    # For simplicity, we'll use approximate scaling\n",
        "    # In practice, you'd use the same scaler from training\n",
        "    scaled_flower = (new_flower - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = nn.predict(scaled_flower)[0]\n",
        "    probabilities = nn.forward_pass(scaled_flower)[0]\n",
        "\n",
        "    print(f\"üå∏ New flower measurements:\")\n",
        "    print(f\"   Sepal length: {sepal_length} cm\")\n",
        "    print(f\"   Sepal width:  {sepal_width} cm\")\n",
        "    print(f\"   Petal length: {petal_length} cm\")\n",
        "    print(f\"   Petal width:  {petal_width} cm\")\n",
        "    print(f\"\\nü§ñ Neural Network Prediction: {species_names[prediction]}\")\n",
        "    print(f\"\\nüìä Confidence scores:\")\n",
        "    for i, (species, prob) in enumerate(zip(species_names, probabilities)):\n",
        "        print(f\"   {species:12s}: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "\n",
        "# Test with some example flowers\n",
        "print(\"Testing with example flowers:\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "predict_flower(5.0, 3.5, 1.5, 0.3)  # Typical Setosa\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "predict_flower(6.0, 3.0, 4.5, 1.5)  # Typical Versicolor\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "predict_flower(7.0, 3.2, 6.0, 2.0)  # Typical Virginica"
      ],
      "metadata": {
        "id": "sfHZkqoIURay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}